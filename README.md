# ğŸš¢ Titanic Survival Prediction 
A hands-on machine learning project to predict whether a passenger survived the Titanic disaster, using demographic, travel, and ticket information.  
This project demonstrates **data preprocessing**, **feature engineering**, **model selection**, and **hyperparameter tuning** to compare different algorithms.

---

## ğŸ“Œ Problem Statement

The Titanic dataset contains passenger details but also a **binary survival label** (`Survived`).  
The goal is to **build and compare models** that predict whether a passenger survived, based on the given attributes.

---

## ğŸ¯ Goal

Predict whether a passenger survived the Titanic disaster, using personal, ticket, and cabin details.

---

## ğŸ“Š Dataset Overview

- **Source:** `Titanic-Dataset.csv` (from [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic))
- **Target:** `Survived` (1 = survived, 0 = did not survive)
- **Features:**
  - Demographic: `Name`, `Sex`, `Age`
  - Ticket & Cabin: `Ticket`, `Fare`, `Cabin`
  - Travel Details: `Pclass`, `SibSp`, `Parch`, `Embarked`
  - Engineered: `Family_members`

---

## ğŸ›  Feature Engineering & Data Preprocessing

- **Handling Missing Values:**
  - `Cabin` â†’ extracted first letter, filled missing with `"Unknown"`
  - `Embarked` â†’ filled missing with most frequent value
  - `Age` â†’ filled missing with median

- **Encoding Categorical Variables:**
  - Used `pd.get_dummies()` for `Sex`, `Embarked`, and `Cabin`

- **Engineered Feature:**
```python
titanic_df['Family_members'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1
```

## ğŸ”„ Project Workflow

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Load Dataset     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Data Cleaning     â”‚
          â”‚  (Missing values)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Feature Engineering â”‚
          â”‚ (Family_members,    â”‚
          â”‚ Cabin extraction)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Encoding Categoricalâ”‚
          â”‚ Variables (dummies) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Train-Test Split   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Model Training &   â”‚
          â”‚ Hyperparameter Tuningâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Model Evaluation   â”‚
          â”‚ (Accuracy, Precisionâ”‚
          â”‚ Recall, F1)        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## 1ï¸âƒ£ Import Required Libraries

```import pandas as pd
from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
```


- pandas â†’ Data loading and manipulation
- sklearn.model_selection â†’ Splitting datasets and cross-validation
- sklearn.linear_model â†’ Logistic Regression model
- sklearn.metrics â†’ Evaluation metrics
- sklearn.ensemble â†’ Random Forest model
- sklearn.tree â†’ Decision Tree model

## 2ï¸âƒ£ Load Dataset

```titanic_df = pd.read_csv('Titanic-Dataset.csv')```

Reads the dataset into a DataFrame for processing, The file should be in the same directory or provide the full path.

## 3ï¸âƒ£ Initial Dataset Overview
```
titanic_df.shape
titanic_df.dtypes
```


- .shape â†’ Returns (rows, columns) count.
- .dtypes â†’ Shows data types for each column.

## 4ï¸âƒ£ Check Missing Values
```
titanic_df.isna().sum()
```


- Identifies columns with missing values.
- Essential before any preprocessing.

##5ï¸âƒ£ Handle Missing Values & Feature Engineering (Cabin & Embarked)
```
# Extract first letter of 'Cabin' to reduce unique categories
titanic_df['Cabin'] = titanic_df['Cabin'].str[0]


# Fill missing Cabin values with 'Unknown'
titanic_df['Cabin'] = titanic_df['Cabin'].fillna('Unknown')

# Fill missing Embarked values with most frequent value
titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])
```

- Cabin â†’ Only keep first letter to represent cabin group. Missing cabins are marked as 'Unknown'.
- Embarked â†’ Filled using mode() (most common port).

## 6ï¸âƒ£ Encode Categorical Variables
```
titanic_df = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked', 'Cabin'])
```

- Converts categorical columns into dummy/indicator variables (0/1 encoding).
- This makes them suitable for ML algorithms.

## 7ï¸âƒ£ Fill Missing Age with Median
```
titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].median())
```

- Median is robust to outliers compared to mean.

## 8ï¸âƒ£ Recheck Missing Values
```
titanic_df.isna().sum()
```

- Confirms there are no more missing values before training.

## 9ï¸âƒ£ Check Class Distribution
```
print(titanic_df['Survived'].value_counts(normalize=True))
```

- Shows survival rate proportions (class balance).

- Helps decide if special handling for imbalance is needed.

##ğŸ”Ÿ Feature Engineering â€” Family Members
```
titanic_df['Family_members'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1
```

- Creates a new feature combining siblings/spouses (SibSp) and parents/children (Parch) plus the passenger themself.

## 1ï¸âƒ£1ï¸âƒ£ Define Features (X) & Target (y)
```
X = titanic_df.drop(['PassengerId', 'SibSp', 'Parch', 'Name', 'Survived', 'Ticket'], axis=1)
y = titanic_df['Survived']
```

- X â†’ All predictors except IDs and irrelevant columns.
- y â†’ Target column (Survived).

## 1ï¸âƒ£2ï¸âƒ£ Train-Test Split
```
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=12, stratify=y
)
```

- 30% test set
- Stratify â†’ Maintains class proportions in both sets.

## 1ï¸âƒ£3ï¸âƒ£ Define Models
```
models = {
    'Logistic Regression': LogisticRegression(max_iter=5000),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}
```

- Logistic Regression â†’ Simple linear model for classification
- Random Forest â†’ Ensemble of decision trees
- Decision Tree â†’ Tree-based classification

## 1ï¸âƒ£4ï¸âƒ£ Hyperparameter Grids
```
params = {
    'Logistic Regression': {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2'],
        'solver': ['saga', 'liblinear']
    },
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5]
    },
    'Decision Tree': {
        'max_depth': [None, 5, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
}
```

- Defines parameter ranges for RandomizedSearchCV tuning

## 1ï¸âƒ£5ï¸âƒ£ Model Training & Evaluation
```
for name, model in models.items():
    kf = KFold(shuffle=True, random_state=42, n_splits=5)
    rs_cv = RandomizedSearchCV(model, params[name], cv=kf, n_iter=10)
    rs_cv.fit(X_train, y_train)
    
    y_pred = rs_cv.predict(X_test)
    
    acc_sc = accuracy_score(y_test, y_pred)
    pre_sc = precision_score(y_test, y_pred)
    rec_sc = recall_score(y_test, y_pred)
    f1_sc = f1_score(y_test, y_pred)
    
    print(f"{name:<20} | Accuracy: {acc_sc:.2f} | Precision: {pre_sc:.2f} | Recall (Sensitivity): {rec_sc:.2f} | F1 Score: {f1_sc:.2f}")
```

- KFold â†’ 5-fold cross-validation for more reliable evaluation
- RandomizedSearchCV â†’ Random search for hyperparameters
  
Prints performance metrics for each model:

- Accuracy â†’ Overall correctness
- Precision â†’ Correct positive predictions
- Recall â†’ Correctly predicted survivors
- F1 Score â†’ Balance between precision and recall
